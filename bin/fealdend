#!/usr/bin/env python
import daemon
import lockfile
import logging
import multiprocessing
import os
import sys
import time

from fealden import searchserver, util, config

"""This daemon listens on a queue for requests, processes those
   requests and then writes the output for webfealden.py"""

try:
    runtime = config.getconfig()
except config.ConfigError:
    sys.stderr.write("Unrecoverable error, exiting: %s\n" % errormsg)
    sys.exit()

context = daemon.DaemonContext(
        working_directory=runtime.get("Locations", "workingdirectory"),
        umask=0o002,
        pidfile=lockfile.FileLock('/var/fealden/fealdend.pid'),
        )

def main():
    default_formatter = logging.Formatter(\
        "%(asctime)s:%(levelname)s:%(name)s:%(message)s")

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.ERROR)
    console_handler.setFormatter(default_formatter)

    error_handler = logging.FileHandler(runtime.get("Locations","log"), "a")
    error_handler.setLevel(logging.DEBUG)
    error_handler.setFormatter(default_formatter)

    rootlogger = logging.getLogger()
    rootlogger.addHandler(console_handler)
    rootlogger.addHandler(error_handler)
    rootlogger.setLevel(logging.ERROR)

    logging.getLogger("fealden.searchserver").setLevel(logging.INFO)
    logging.getLogger("fealden.backtracking").setLevel(logging.INFO)
    logging.getLogger("fealden.unafold").setLevel(logging.DEBUG)

    #handler = logging.FileHandler("/var/fealden/fealdend.log")
    #formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # Create main request queue to be shared by all workers,
    # this is, obviously, thread/process safe, unlike DirectoryQueue.
    request_q = multiprocessing.Queue()

    # Launch search server
    numprocs = 1
    rootlogger.info("fealdend: launching searchserver with %d searchworkers" % numprocs)
    searchserver.start(request_q, parent_pid=os.getpid(), numprocs=numprocs)
    
    # Now in this process we will centrally manage all the requests
    # coming in for searches from any number of front ends. Each
    # request will be verified and then written to a request queue
    # that all workers performing searches will be watching.
    workqueue_dir = runtime.get("Locations", "workqueue")
    rootlogger.info("fealdend: opening workqueue %s" % (workqueue_dir))
    workqueue = util.DirectoryQueue(workqueue_dir)
    while True:
        # Read until queue is empty, blocking
        request = workqueue.get()
        if request.valid() and isinstance(request, util.RequestElement):
            rootlogger.info("fealdend: adding %s for %s to request_q" %
                        (request.command, request.recognition))
            request_q.put(request)
        else:
            rootlogger.info("fealdend: received bad request on request_q: %s" % request)

if __name__ == '__main__':
    #with context:
    main()
    
