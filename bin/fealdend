#!/usr/bin/env python
import daemon
import lockfile
import logging
import multiprocessing
import os
import signal
import sys
import time

from fealden import searchserver, util, config

"""This daemon listens on a queue for requests, processes those
   requests and then writes the output for webfealden.py"""


# Try to read in our configuration file, which mush exist.
try:
    runtime = config.getconfig()
except config.ConfigError:
    sys.stderr.write("Unrecoverable error, exiting: %s\n" % errormsg)
    sys.exit()

# Configure the daemon context
context = daemon.DaemonContext(
    working_directory=runtime.get("Locations", "workingdirectory"),
    umask=0o002,
    detach_process=False,
    pidfile=lockfile.FileLock('/var/fealden/fealdend.pid'),
    stderr=sys.stderr
    )

def test_handler(signum, frame):
    logger = logging.getLogger(__name__)
    logger.error("(%s) Got sigterm" % os.getpid())
    sys.exit()


context.signal_map = {
    signal.SIGTERM: test_handler
    }


def main():

    default_formatter = logging.Formatter(\
        "%(asctime)s:%(levelname)s:%(name)s:%(message)s")

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.ERROR)
    console_handler.setFormatter(default_formatter)

    error_handler = logging.FileHandler(runtime.get("Locations","log"), "a")
    error_handler.setLevel(logging.DEBUG)
    error_handler.setFormatter(default_formatter)

    rootlogger = logging.getLogger()
    rootlogger.addHandler(console_handler)
    rootlogger.addHandler(error_handler)
    #rootlogger.setLevel(logging.ERROR)

    logger = logging.getLogger(__name__)

    logging.getLogger("fealden.searchserver").setLevel(logging.INFO)
    logging.getLogger("fealden.backtracking").setLevel(logging.INFO)
    logging.getLogger("fealden.unafold").setLevel(logging.DEBUG)
    logging.getLogger("fealdend").setLevel(logging.DEBUG)

    #handler = logging.FileHandler("/var/fealden/fealdend.log")
    #formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # Create main request queue to be shared by all workers,
    # this is, obviously, thread/process safe, unlike DirectoryQueue.
    request_q = multiprocessing.Queue()

    # Launch search server
    numprocs = 1
    logger.info("fealdend: launching searchserver with %d searchworkers" % numprocs)
    searchserver.start(request_q, parent_pid=os.getpid(), numprocs=numprocs)
    
    # Now in this process we will centrally manage all the requests
    # coming in for searches from any number of front ends. Each
    # request will be verified and then written to a request queue
    # that all workers performing searches will be watching.
    workqueue_dir = runtime.get("Locations", "workqueue")
    logger.info("fealdend: opening workqueue %s" % (workqueue_dir))
    workqueue = util.DirectoryQueue(workqueue_dir)
    while True:
        # Read until queue is empty, blocking
        request = workqueue.get()
        if request.valid() and isinstance(request, util.RequestElement):
            logger.info("fealdend: adding %s for %s to request_q" %
                        (request.command, request.recognition))
            request_q.put(request)
        else:
            logger.info("fealdend: received bad request on request_q: %s" % request)

if __name__ == '__main__':
    with context:
        main()
    
